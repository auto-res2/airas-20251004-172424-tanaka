
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Strictly proper scoring rules such as Brier or Spherical improve calibration and downstream generation quality when used for fine-tuning, but they converge slowly and give worse perplexity than cross-entropy (CE) when training from scratch.  The root cause is their small initial gradients; models learn the rough shape of the distribution too slowly.",
    "Methods": "Curriculum Mixture of Scoring Rules (CMSR)\n1. Loss definition  L_t = (1−w_t) · CE + w_t · Spherical\n   – CE = standard token-level cross-entropy.\n   – Spherical = token-level strictly proper score already implemented in prior work.\n2. Weight schedule  w_t ∈ [0,1]\n   – Begin with w_0 = 0 (pure CE).\n   – Increase linearly or with cosine schedule to w_T = 1 over the first K% of total training steps (e.g., K = 30%).  After that, keep w_t = 1.\n3. Motivation\n   – Early CE component supplies large informative gradients, speeding convergence.\n   – Gradually shifting to Spherical retains its superior calibration and generation quality.\n   – No new hyper-parameters beyond K (schedule length) and the schedule form; can reuse existing optimizer settings.",
    "Experimental Setup": "Model: 125 M-parameter transformer (same as GPT-Neo-125M) trained from scratch.\nData: WikiText-103 language-modeling split.\nBaselines: (a) CE only, (b) Spherical only (token version of prior paper).\nProposed: CMSR with K = 30 % linear schedule.\nTraining: 100k steps, batch 256, AdamW, learning-rate 3e-4, other hyper-parameters identical across runs.\nEvaluation:\n1. Perplexity (lower is better).\n2. Expected Calibration Error (ECE) over next-token probabilities (lower is better).\n3. Downstream generation: CNN/DailyMail summarization fine-tune for 1 epoch, ROUGE-L.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef spherical_loss(logits, target):\n    # logits: [B, V], target: [B]\n    prob = logits.softmax(dim=-1)\n    true_prob = prob[torch.arange(prob.size(0)), target]\n    return 1 - true_prob / prob.norm(dim=-1)\n\nclass CMSRLoss(torch.nn.Module):\n    def __init__(self, total_steps, warmup_ratio=0.3, schedule='linear'):\n        super().__init__()\n        self.total_steps = total_steps\n        self.switch_steps = int(total_steps * warmup_ratio)\n        self.schedule = schedule\n        self.register_buffer('step', torch.tensor(0.0))\n\n    def forward(self, logits, target):\n        ce = F.cross_entropy(logits, target, reduction='none')\n        sph = spherical_loss(logits, target)\n        t = self.step.item()\n        if t < self.switch_steps:\n            if self.schedule == 'linear':\n                w = t / self.switch_steps\n            else:  # cosine\n                w = 0.5 * (1 - torch.cos(torch.tensor(t / self.switch_steps * 3.141592)))\n        else:\n            w = 1.0\n        self.step += 1\n        return ((1 - w) * ce + w * sph).mean()",
    "Expected Result": "• CE will converge fastest but give poorer calibration (ECE≈7%).\n• Spherical will converge slowest, giving perplexity ~5% worse than CE but best ECE (~3%).\n• CMSR is expected to match CE perplexity within 1% while retaining most of Spherical’s calibration benefit (ECE≈3.5%).\n• On downstream summarization, CMSR fine-tuned models should gain +0.5 ROUGE-L over CE baseline and match Spherical.",
    "Expected Conclusion": "A simple curriculum mixture of cross-entropy and a strictly proper scoring rule eliminates the slow-convergence drawback of alternative losses while preserving their calibration and generation advantages.  Because it only requires a two-line change to the loss function and a lightweight schedule, CMSR can be immediately adopted in existing LLM training pipelines to yield better calibrated and equally performant models without additional tuning."
}

# MODEL LIST
{
    "Large Language Models": {
        "Llama-4-Scout-17B-16E": "Llama 4, developed by Meta, is a new generation of natively multimodal AI models that leverage a Mixture-of-Experts (MoE) architecture to achieve state-of-the-art performance in both text and image understanding. Marking the beginning of a new era for the Llama ecosystem, the series introduces two models: Llama 4 Scout, a 17-billion-parameter model with 16 experts, and Llama 4 Maverick, also with 17 billion parameters but incorporating 128 experts. These auto-regressive language models employ early fusion to enable seamless multimodal processing, allowing them to integrate text and image information natively.",
        "Llama-4-Maverick-17B-128E": "The Llama 4 collection, developed by Meta, represents a new generation of natively multimodal AI models designed to enable both text and multimodal experiences. By leveraging a Mixture-of-Experts (MoE) architecture, these models deliver industry-leading performance in understanding text and images. Marking the beginning of a new era for the Llama ecosystem, the series introduces two efficient models: Llama 4 Scout, a 17-billion-parameter model with 16 experts, and Llama 4 Maverick, also with 17 billion parameters but featuring 128 experts. Built as auto-regressive language models, the Llama 4 series incorporates early fusion to achieve seamless and native multimodality.",
        "Qwen3-0.6B": "Qwen3-0.6B is a compact causal language model with 0.6B parameters, offering dense and MoE variants, improved reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and 32K context length.",
        "Qwen3-1.7B": "Qwen3-1.7B is a next-generation causal language model with 1.7B parameters, offering dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and long-context processing up to 32K tokens.",
        "Qwen3-4B": "Qwen3 is the latest generation of large language models, featuring both dense and MoE variants with enhanced reasoning, instruction-following, agent capabilities, and multilingual support, including seamless mode switching, superior alignment, and long-context processing up to 131K tokens.",
        "Qwen3-8B": "Qwen3-8B is an advanced causal language model with 8.2B parameters, featuring dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and long-context processing up to 131K tokens.",
        "Qwen3-14B": "Qwen3-14B is a large-scale causal language model with 14.8B parameters, offering dense and MoE variants, advanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and extended context handling up to 131K tokens.",
        "Qwen3-32B": "Qwen3-32B is a powerful causal language model with 32.8B parameters, featuring dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, advanced agent capabilities, multilingual support, and long-context processing up to 131K tokens.",
        "DeepSeek-v3": "DeepSeek-V3 is a 671B-parameter MoE language model (37B active per token) featuring MLA and DeepSeekMoE architectures, auxiliary-loss-free load balancing, and multi-token prediction, trained on 14.8T tokens with efficient GPU usage, achieving performance comparable to top closed-source models while maintaining stable training.",
        "DeepSeek-V3.1": "DeepSeek-V3.1 extends DeepSeek-V3 with larger long-context training (630B tokens at 32K and 209B tokens at 128K) and adopts FP8 data formats for efficiency and compatibility.",
        "DeepSeek-V3.2-Exp": "DeepSeek-V3.2-Exp, built on V3.1-Terminus, introduces Sparse Attention to improve training and inference efficiency for long-context processing as part of ongoing research into more efficient transformer architectures.",
        "gpt-oss-20b": "The gpt-oss series, introduced by OpenAI, consists of open-weight models designed to support powerful reasoning, agentic tasks, and a wide range of developer use cases. Two versions are being released: gpt-oss-120b, a 117-billion-parameter model with 5.1 billion active parameters optimized for production-level, general-purpose, high-reasoning tasks that can fit into a single 80GB GPU such as the NVIDIA H100 or AMD MI300X; and gpt-oss-20b, a 21-billion-parameter model with 3.6 billion active parameters intended for lower latency, as well as local or specialized applications. Both models were trained using OpenAI’s harmony response format and must be used with this format to function correctly.",
        "gemma-3-1b-it": "Gemma is a family of lightweight open models from Google, built on the same research as Gemini. The latest Gemma 3 models are multimodal, supporting both text and image inputs with text generation outputs. They feature a 128K context window (32K for the 1B model), multilingual support in over 140 languages, and come in multiple sizes, making them suitable for tasks such as question answering, summarization, reasoning, and image understanding. Their smaller size allows deployment on laptops, desktops, or personal cloud infrastructure, broadening access to advanced AI. Inputs include text and images normalized to 896×896 resolution, while outputs are generated text with up to 8192 tokens of context.",
        "gemma-3-4b-it": "Gemma is a family of lightweight open models from Google, built on the same research behind the Gemini models. The latest Gemma 3 models are multimodal, capable of processing both text and images as input and generating text as output, with open weights for pre-trained and instruction-tuned variants. They offer a 128K context window (32K for the 1B model), support over 140 languages, and come in more sizes than earlier versions, making them suitable for tasks like question answering, summarization, reasoning, and image understanding. Thanks to their smaller size, Gemma models can run on laptops, desktops, or personal cloud setups, expanding access to advanced AI. Inputs include text and images (normalized to 896×896 and encoded to 256 tokens each), while outputs are generated text with up to 8192 tokens.",
        "gemma-3-27b-it": "Gemma is a family of lightweight open models from Google, built on the same research as the Gemini models. The Gemma 3 series is multimodal, able to take both text and image inputs and generate text outputs, with open weights available for both pre-trained and instruction-tuned versions. They feature a 128K context window (32K for the 1B model), multilingual support in more than 140 languages, and come in a wider range of sizes than previous releases. Well-suited for tasks such as question answering, summarization, reasoning, and image understanding, Gemma models are compact enough to run on laptops, desktops, or personal cloud setups, making advanced AI more broadly accessible. Inputs include text strings or images normalized to 896×896 and encoded into 256 tokens each, while outputs are generated text of up to 8192 tokens."
    },
    "Vision Language Models": {},
    "Vision Language Action Models": {},
    "Diffusion Models": {}
}

# DATASET LIST
{
    "Text Datasets": {
        "alpaca-cleaned": "",
        "databricks-dolly-15k": ""
    },
    "Image Datasets": {
        "ImageNet": "",
        "CIFAR-10": ""
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- new_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - List all hyperparameters to be explored, including only their names, in a list format.
Output:
{
    "experiment_summary": "We will train three identical 125-million-parameter Transformer language models from scratch on WikiText-103.  The baseline model is optimized with pure token-level cross-entropy (CE).  The proposed model is optimized with Curriculum Mixture of Scoring Rules (CMSR), which linearly interpolates CE and Spherical loss during the first K % of training steps and then uses pure Spherical loss.  A second comparative model trained with pure Spherical loss is also run to isolate the effect of the curriculum.  All runs share the same optimizer (AdamW), learning rate, batch size, and total steps to ensure a fair comparison.  After pre-training, the checkpoints are evaluated on the WikiText-103 validation set for perplexity and Expected Calibration Error; then each model is fine-tuned for one epoch on CNN/DailyMail summarization and evaluated with ROUGE-L.  Superior calibration and equal or better perplexity/ROUGE would demonstrate the advantage of CMSR.",
    "evaluation_metrics": [
        "Perplexity",
        "Expected Calibration Error",
        "ROUGE-L"
    ],
    "models_to_use": [
        "GPT-Neo-125M"
    ],
    "datasets_to_use": [
        "WikiText-103"
    ],
    "new_method": "Curriculum Mixture of Scoring Rules (CMSR).  Objective: combine fast convergence of cross-entropy with the calibration benefits of strictly proper scoring rules.  Theory: CE provides large informative gradients early in training; Spherical loss is strictly proper and yields better calibrated probabilities but starts with very small gradients when the model is untrained.  Algorithm: (1) at step t compute CE and Spherical token losses; (2) compute weight w_t that increases from 0 to 1 over the first K % of total steps, via linear or cosine schedule; (3) total loss L_t = (1−w_t)*CE + w_t*Spherical; (4) after switch_steps, set w_t = 1 (pure Spherical).  Implementation requires only a small wrapper around the existing loss function (provided Python code).  The method introduces one hyperparameter K and an optional choice of schedule shape.",
    "comparative_methods": [
        "Cross-Entropy loss (CE)"
    ],
    "hyperparameters_to_search": [
        "Schedule Length K (%)",
        "Schedule Type (linear or cosine)",
        "Learning Rate"
    ]
}
