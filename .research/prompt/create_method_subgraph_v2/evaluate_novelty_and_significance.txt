
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "Strictly proper scoring rules such as Brier or Spherical improve calibration and downstream generation quality when used for fine-tuning, but they converge slowly and give worse perplexity than cross-entropy (CE) when training from scratch.  The root cause is their small initial gradients; models learn the rough shape of the distribution too slowly.",
    "Methods": "Curriculum Mixture of Scoring Rules (CMSR)\n1. Loss definition  L_t = (1−w_t) · CE + w_t · Spherical\n   – CE = standard token-level cross-entropy.\n   – Spherical = token-level strictly proper score already implemented in prior work.\n2. Weight schedule  w_t ∈ [0,1]\n   – Begin with w_0 = 0 (pure CE).\n   – Increase linearly or with cosine schedule to w_T = 1 over the first K% of total training steps (e.g., K = 30%).  After that, keep w_t = 1.\n3. Motivation\n   – Early CE component supplies large informative gradients, speeding convergence.\n   – Gradually shifting to Spherical retains its superior calibration and generation quality.\n   – No new hyper-parameters beyond K (schedule length) and the schedule form; can reuse existing optimizer settings.",
    "Experimental Setup": "Model: 125 M-parameter transformer (same as GPT-Neo-125M) trained from scratch.\nData: WikiText-103 language-modeling split.\nBaselines: (a) CE only, (b) Spherical only (token version of prior paper).\nProposed: CMSR with K = 30 % linear schedule.\nTraining: 100k steps, batch 256, AdamW, learning-rate 3e-4, other hyper-parameters identical across runs.\nEvaluation:\n1. Perplexity (lower is better).\n2. Expected Calibration Error (ECE) over next-token probabilities (lower is better).\n3. Downstream generation: CNN/DailyMail summarization fine-tune for 1 epoch, ROUGE-L.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef spherical_loss(logits, target):\n    # logits: [B, V], target: [B]\n    prob = logits.softmax(dim=-1)\n    true_prob = prob[torch.arange(prob.size(0)), target]\n    return 1 - true_prob / prob.norm(dim=-1)\n\nclass CMSRLoss(torch.nn.Module):\n    def __init__(self, total_steps, warmup_ratio=0.3, schedule='linear'):\n        super().__init__()\n        self.total_steps = total_steps\n        self.switch_steps = int(total_steps * warmup_ratio)\n        self.schedule = schedule\n        self.register_buffer('step', torch.tensor(0.0))\n\n    def forward(self, logits, target):\n        ce = F.cross_entropy(logits, target, reduction='none')\n        sph = spherical_loss(logits, target)\n        t = self.step.item()\n        if t < self.switch_steps:\n            if self.schedule == 'linear':\n                w = t / self.switch_steps\n            else:  # cosine\n                w = 0.5 * (1 - torch.cos(torch.tensor(t / self.switch_steps * 3.141592)))\n        else:\n            w = 1.0\n        self.step += 1\n        return ((1 - w) * ce + w * sph).mean()",
    "Expected Result": "• CE will converge fastest but give poorer calibration (ECE≈7%).\n• Spherical will converge slowest, giving perplexity ~5% worse than CE but best ECE (~3%).\n• CMSR is expected to match CE perplexity within 1% while retaining most of Spherical’s calibration benefit (ECE≈3.5%).\n• On downstream summarization, CMSR fine-tuned models should gain +0.5 ROUGE-L over CE baseline and match Spherical.",
    "Expected Conclusion": "A simple curriculum mixture of cross-entropy and a strictly proper scoring rule eliminates the slow-convergence drawback of alternative losses while preserving their calibration and generation advantages.  Because it only requires a two-line change to the loss function and a lightweight schedule, CMSR can be immediately adopted in existing LLM training pipelines to yield better calibrated and equally performant models without additional tuning."
}

# Related Works
{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper addresses the limitation of language generation models primarily relying on the logarithmic score (cross-entropy loss) due to the locality constraint on strictly proper scoring rules in exponentially large sample spaces. It proposes a novel strategy to adapt any non-local strictly proper scoring rules, such as the Brier score and Spherical score, to language generation by distributing them at the token level. The research also introduces a score smoothing technique to enable label smoothing for arbitrary scoring rules. Experimental results demonstrate that simply substituting the loss function with Brier or Spherical scores, without hyperparameter adjustment, substantially improves model generation capabilities, even scaling up to large language models (LLMs) like LLaMA-7B and LLaMA-13B, particularly during fine-tuning.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules for language generation. This is achieved by distributing the scoring rule at the token level, transforming the sequence prediction problem into a series of conditional token prediction tasks where the sample space is reduced to the vocabulary size. The paper defines the loss function as the negative sum of token-level scores, ensuring minimization only when model probabilities match true conditional probabilities. Two classic non-local strictly proper scoring rules, the Brier score and the Spherical score, are implemented as alternative loss functions. Additionally, a general score smoothing technique is introduced, defined as `Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · Σj S(p, j)`. To address the issue of smoothing terms being ignored for some scores, an enhanced masked logarithmic score `Sϵ log(p, i) = Sϵ(p, i) + ϵ/m Σj 1{pj < ϵ/m}log pj` is proposed to incentivize models to produce the desired smooth distribution. Models are trained from scratch or fine-tuned by replacing the standard log-likelihood loss with these alternative scoring rule losses.",
    "Experimental Setup": "The method was evaluated on machine translation and abstractive summarization tasks. For machine translation, WMT14 English-French (En-Fr), WMT14 English-German (En-De), and TED bilingual datasets were used, with BLEU as the metric. For abstractive summarization, the CNN/DailyMail dataset was used, with ROUGE-1, ROUGE-2, and ROUGE-L as metrics. Transformer-base and Transformer-big models were employed for these tasks. Large Language Models (LLaMA-7B and LLaMA-13B) were used as foundation models for instruction tuning on the Alpaca dataset (52K instruction-following demonstrations). LLM performance was also evaluated on WMT22 translation tasks (Chinese-to-English, English-to-Chinese, German-to-English, English-to-German) and CNN/DailyMail summarization, as well as open-ended question answering capabilities using MT-bench. Training involved both training from scratch and fine-tuning pre-trained models, keeping hyperparameters fixed during fine-tuning. Beam search with varying beam sizes and length penalties was used for decoding. The impact of pseudo-spherical and α-power scores with different α values was also investigated.",
    "Limitations": "The study found that when training language generation models from scratch, alternative strictly proper scoring rules (Brier, Spherical) exhibit slower convergence and generally lower performance compared to the logarithmic score. This is hypothesized to be due to their distinct learning dynamics, such as the logarithmic score's larger initial gradient, and the use of hyperparameter settings optimized for the logarithmic score. Furthermore, the proposed score smoothing technique might be less effective for certain scoring rules (e.g., spherical score) due to the loss function being relatively flat around the optimal point, causing the smoothing term to be almost disregarded without the masked logarithmic score enhancement. The investigation into using different scoring rules as objectives for beam search was also noted as not exhaustive.",
    "Future Research Directions": "The paper suggests several future research avenues: exploring other strictly proper scoring rules for potential superior performance during pre-training or fine-tuning, investigating the underlying factors contributing to performance differences among various strictly proper scores, developing additional metrics or properties to determine a score's suitability for training language generation models, and exploring the use of these scores as evaluation metrics (similar to Perplexity) for assessing the calibration of language generation models. Further investigation into the application of other strictly proper scoring rules as objectives for beam search is also an open direction.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This research addresses the limitation of human creativity in discovering new preference optimization algorithms for Large Language Models (LLMs) by introducing an LLM-driven objective discovery pipeline. It automatically generates and evaluates novel state-of-the-art preference optimization algorithms without expert human intervention. The key finding is the discovery of Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. DiscoPOP achieves strong performance across multiple held-out evaluation tasks, including multi-turn dialogue (AlpacaEval 2.0), controlled sentiment generation (IMDb), and summarization (TL;DR). The work also provides an initial analysis of DiscoPOP, noting its non-convex nature and a weighted sum of logistic and exponential losses.",
    "Methodology": "The core methodology involves an LLM-driven objective discovery pipeline, a form of meta-optimization, to uncover novel learning algorithms. An LLM (GPT-4) is iteratively prompted to propose and implement new preference optimization loss functions in Python code. The process begins with 'burning-in' the LLM with established objective functions and their performance. Each proposed function undergoes unit tests for validity. Valid functions are used to fine-tune an LLM, and its performance on a predefined downstream validation task (MT-Bench score) is evaluated and fed back to the LLM as in-context examples. This iterative refinement allows the LLM to synthesize new candidate loss functions by exploring variations of successful formulas and entirely new formulations. The loss functions operate on the log ratio difference (ρ) between chosen and rejected completions from the policy and reference models.",
    "Experimental Setup": "The LLM-driven discovery was performed using GPT-4 for proposing objective functions. For training, a 'zephyr-7b-gemma-sft' model (a 7-billion parameter Gemma model supervised fine-tuned on the 'deita-10k-v0-sft' dataset) was used as the base LLM. The models were trained on the 'Argilla DPO Mix 7K' pairwise preference dataset with fixed hyperparameters (β=0.05, learning rate 5e-7, bfloat16, 2 epochs, batch size 2, gradient accumulation 8, cosine scheduler, AdamW). Training was conducted on 8 Nvidia A100 GPUs. Approximately 100 objective functions were evaluated. The discovery task used MT-Bench for evaluation, where GPT-4 assessed the quality of trained model responses. Held-out evaluations included Alpaca Eval 2.0 (single-turn dialogue, comparing win rates against GPT-4 or SFT base model), summarization on a 10% subsample of the Reddit TL;DR dataset (evaluated with Alpaca Eval 2.0 library and custom GPT-4 annotator), and positive sentiment generation on the IMDb dataset (using a GPT-2 model fine-tuned on IMDb, evaluated with a pre-trained sentiment classifier for rewards and KL-Divergence, with a β sweep from 0.01 to 5).",
    "Limitations": "The current approach has several limitations. First, the method for generating LLM objective proposals is still nascent, with initial exploratory experiments using temperature sampling or performance-based sorting not yielding significant improvements. Second, the highest-performing discovered loss (DiscoPOP) repurposes the β parameter, making it influence both the functional behavior and the KL penalty, which can lead to training instability. Third, the reliance on closed-source models like GPT-4 for code generation limits reproducibility and incurs high costs. Finally, DiscoPOP specifically struggles to converge when the β value is either too low (β ≤ 0.01) or too high (β ≥ 2.5), potentially due to not observing these β ranges during the discovery process, getting stuck in local minima, or collapsing due to large gradients in non-convex regions.",
    "Future Research Directions": "Future research could focus on improving the effectiveness of LLM objective proposals by leveraging more training run information, such as providing entire learning curve plots to Visual Language Models, or by automatically tuning instruction prompt templates and meta-meta-optimizing the LLM prompt. Another direction is to study multi-parameter loss functions, allowing for separate tuning of multiple floating-point parameters within the objective. Addressing DiscoPOP's convergence issues with extreme β values through gradient clipping or objective reformulation is also suggested. Lastly, the work motivates using the discovered models themselves to generate code, leading to code-level self-improvement in language models, which would enhance reproducibility and reduce costs associated with closed-source models.",
    "Experiment Code": "def init_archive(config):\n    if config[\"B_PARAMS\"] == 2:\n        fitnesses = [\n            4.75,  # DPO\n            4.72,  # HINGE\n            4.69,  # IPO\n            4.68,  # KTO\n        ]\n    else:\n        fitnesses = [\n            7.887500,  # DPO\n            7.881250,  # HINGE\n            7.84,  # IPO\n            7.603125,  # KTO\n        ]\n    archive = []\n    archive.append(\n        {  # DPO\n            \"code\": \"\"\"\ndef logistic_log_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = -F.logsigmoid(self.beta * logits)\n    return losses\n    \"\"\",\n            \"fitness\": None,\n        }\n    )\n    archive.append(\n        {  # HINGE\n            \"code\": \"\"\"\ndef hinge_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = torch.relu(1 - self.beta * logits)\n    return losses\n    \"\"\",\n            \"fitness\": None,\n        }\n    )\n    archive.append(\n        {  # IPO\n            \"code\": \"\"\"\ndef ipo_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = (logits - 1 / (2 * self.beta)) ** 2\n    return losses\n    \"\"\",\n            \"fitness\": None,\n        }\n    )\n    archive.append(\n        {  # KTO\n            \"code\": \"\"\"\ndef kto_pair_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n    rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n\n    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n    # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.\n    losses = torch.cat(\n        (\n            1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),\n            1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),\n        ),\n        0,\n    )\n    return losses\n    \"\"\",\n            \"fitness\": None,\n        }\n    )\n\n    for archive_entry, new_fitness in zip(archive, fitnesses):\n        archive_entry[\"fitness\"] = new_fitness\n\n    return archive\n\ndef validate_code(code: str) -> bool:\n    # Run code through test\n    try:\n        # Namespace dictionary to hold the execution context\n        namespace = {}\n\n        # Execute the function definition string within the provided namespace\n        exec(code, globals(), namespace)\n\n        names = list(namespace.keys())\n        if len(names) != 1:\n            return False, f\"{len(names)} things in namespace. Please only provide 1\"\n        func = namespace[names[0]]\n        if not callable(func):\n            return False, f\"{func} is not callable\"\n\n        # Create a class to hold the sigmoid_loss function\n        class LossModel:\n            def __init__(self, beta):\n                self.beta = beta\n\n        # Add the function to the class\n        setattr(LossModel, \"epo_loss\", func)\n\n        model = LossModel(beta=0.05)\n\n        # Define input tensors with requires_grad to check gradients\n        policy_chosen_logps = torch.randn(10, requires_grad=True)\n        policy_rejected_logps = torch.randn(10, requires_grad=True)\n        reference_chosen_logps = torch.randn(10, requires_grad=True)\n        reference_rejected_logps = torch.randn(10, requires_grad=True)\n\n        # Compute the loss\n        loss = model.epo_loss(\n            policy_chosen_logps,\n            policy_rejected_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n        )\n\n        # Check for NaNs in the output\n        if torch.isnan(loss).any():\n            return False, \"Loss contains NaNs\"\n\n        # Check the shape of the output\n        if loss.shape != (10,):\n            return (\n                False,\n                f\"Expected loss shape to be per input (e.g. (10,)), got {loss.shape}\",\n            )\n\n        # Backward pass to compute gradients\n        loss.mean().backward()\n\n        # Check for NaNs in gradients\n        for param in [\n            policy_chosen_logps,\n            policy_rejected_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n        ]:\n            if torch.isnan(param.grad).any():\n                return False, f\"Gradient for {param} contains NaNs\"\n\n        return True, \"\"\n\n    except Exception as e:\n        return False, str(e)\n\ndef train_gpo(info, config):\n    # STORE CODE\n    with open(f\"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/tests.json\", \"r\") as f:\n        tests = json.load(f)\n    info[\"name\"] = info[\"name\"].lower().replace(\" \", \"_\")\n    for test in tests:  # MAKE NAME UNIQUE.\n        if test[\"name\"] == info[\"name\"]:\n            info[\"name\"] = f\"{info['name']}_{len(tests)}\"\n\n    tests.append(\n        {\n            \"name\": info[\"name\"],\n            \"code\": info[\"code\"],\n        }\n    )\n    with open(f\"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/tests.json\", \"w\") as f:\n        json.dump(tests, f, indent=4)\n    # Define the command as a list of arguments\n    if config[\"NUM_GPUS\"] == 4:\n        command = [\n            \"accelerate\",\n            \"launch\",\n            \"--config_file\",\n            \"recipes/accelerate_configs/deepspeed_zero3.yaml\",\n            \"--num_processes=4\",\n            \"scripts/run_gpo.py\",\n            f\"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/config_full.yaml\",\n            \"--gradient_accumulation_steps=16\",\n        ]\n    else:\n        command = [\n            \"accelerate\",\n            \"launch\",\n            \"--config_file\",\n            \"recipes/accelerate_configs/deepspeed_zero3.yaml\",\n            \"scripts/run_gpo.py\",\n            f\"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/config_full.yaml\",\n        ]\n\n    # Set environment variables directly in the subprocess call\n    env = dict(\n        os.environ, ACCELERATE_LOG_LEVEL=\"info\"\n    )  # Copy current environment and add/modify\n\n    # Execute the command\n    result = subprocess.run(command, env=env)\n    if result.returncode == 0:\n        return True, \"\"\n    return False, f\"Failed with return code: {result.returncode}\\n{result.stderr}\"\n\ndef evaluate_gpo(info, config):\n    model_id = f\"zephyr-{config['B_PARAMS']}b-g-{info['name']}\"\n    # Command to run the script using the specified Python interpreter\n    command = [\n        \"python\",\n        \"gen_model_answer.py\",\n        \"--model-path\",\n        f\"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}\",\n        \"--model-id\",\n        model_id,\n        \"--num-gpus-total\",\n        str(config[\"NUM_GPUS\"]),\n    ]\n\n    cwd = config[\"LLM_JUDGE_DIR\"]\n    result = subprocess.run(command, cwd=cwd)\n    if result.returncode != 0:\n        return (\n            False,\n            f\"Gen Model failed with return code: {result.returncode}\\n{result.stderr}\",\n        )\n\n    command = [\n        \"python\",\n        \"gen_judgment.py\",\n        \"--model-list\",\n        model_id,\n        \"--parallel\",\n        \"4\",\n    ]\n    result = subprocess.run(command, cwd=cwd)\n    if result.returncode != 0:\n        return (\n            False,\n            f\"Gen Judgemnt failed with return code: {result.returncode}\\n{result.stderr}\",\n        )\n    input_file = osp.join(cwd, \"data/mt_bench/model_judgment/gpt-4_single.jsonl\")\n\n    print(f\"Input file: {input_file}\")\n    df_all = pd.read_json(input_file, lines=True)\n    df = df_all[[\"model\", \"score\", \"turn\"]]\n    df = df[df[\"score\"] != -1]\n    df = df[df[\"model\"].isin([model_id])]\n\n    print(\"\\n########## First turn ##########\")\n    df_1 = df[df[\"turn\"] == 1].groupby([\"model\", \"turn\"]).mean()\n    print(df_1.sort_values(by=\"score\", ascending=False))\n\n    print(\"\\n########## Second turn ##########\")\n    df_2 = df[df[\"turn\"] == 2].groupby([\"model\", \"turn\"]).mean()\n    print(df_2.sort_values(by=\"score\", ascending=False))\n\n    print(\"\\n########## Average ##########\")\n    df_3 = df[[\"model\", \"score\"]].groupby([\"model\"]).mean()\n    print(df_3.sort_values(by=\"score\", ascending=False))\n    model_score = df_3.loc[model_id][\"score\"]\n\n    if model_score >= 7.9:\n        print(\"UPLOADING\")\n        command = [\n            \"huggingface-cli\",\n            \"upload\",\n            f\"zephyr-{config['B_PARAMS']}b-gemma-{info['name']}\",\n            \".\",\n            \".\",\n        ]\n\n        cwd = f\"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}\"\n        result = subprocess.run(command, cwd=cwd)\n        if result.returncode != 0:\n            print(\"UPLOAD FAILED\")\n            print(\n                f\"Upload Model failed with return code: {result.returncode}\\n{result.stderr}\"\n            )\n\n    elif not model_score > 7.75:\n        print(\"DELETING\")\n        command = [\n            \"rm\",\n            \"-rf\",\n            f\"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}\",\n        ]\n        result = subprocess.run(command)\n        if result.returncode != 0:\n            print(\n                f\"Delete Model failed with return code: {result.returncode}\\n{result.stderr}\"\n            )\n\n    return True, model_score\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    assert args.num_gpus in [4, 8], \"NUM GPUS must be 4 or 8\"\n    print(f\"NUM GPUS: {args.num_gpus}\")\n    assert args.b_params in [2, 7], \"MUST BE 2B OR 7B\"\n    print(f\"PARAMS: {args.b_params}B\")\n\n    now = str(datetime.now()).replace(\" \", \"_\")\n    if not args.no_logging:\n        save_dir = f\"runs/{now}\"\n        os.mkdir(save_dir)\n\n    config = {\n        \"NUM_GENERATIONS\": args.num_generations,\n        \"SAVE_DIR\": save_dir,\n        \"NOW\": now,\n        \"NUM_GPUS\": args.num_gpus,\n        \"B_PARAMS\": args.b_params,\n        \"RESUME\": args.resume,\n        \"LLM_JUDGE_DIR\": args.llm_judge_dir,\n    }\n    system_prompt = \"\"\"\nYou are a machine learning researcher who is testing out different RLHF loss functions. When you respond, output a JSON where the first key (\"thought\") corresponds to your thought process when designing the next function. The second key (\"name\") corresponds to the name of your next function. Finally, the last key (\"code\") corresponds to the exact python code that you would like to try. Here is an example:\n\n{\"thought\": \"Based on the previous outputs, I should try the direct preference optimization algorithm.\",\n\"name\": \"dpo\",\n\"code\": \"def sigmoid_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = -F.logsigmoid(self.beta * logits)\n    return losses\"\n}\n\nYou are deeply familiar with binary classification losses from the literature. Be creative and reference prior literature when possible.\n\nYou must use the exact function interface used above. Feel free to define extra hyperparameters within your function as constants. Do not make them attributes of self.\n\nNote that `self.beta = 0.05`.\n\nRLHF loss functions train on a dataset of pairs of preferred and rejected completions.\n`policy_chosen_logps` refers to the policy's log probabilities of the preferred completion, and `policy_rejected_logps` refers to the policy's log probabilities of the rejected completion.\n`reference_chosen_logps` and `reference_rejected_logps` refer to the same for the reference (base) model.\n\nThe user will then return to you a fitness that corresponds to the performance of the resulting model on a downstream task. Your goal is to maximize performance.\n\"\"\"\n    archive = init_archive(config)\n    first_prompt = f\"\"\"\nHere are some results we've obtained: \\n{archive}\n\nPlease generate the next one.\n\"\"\"\n    if not args.resume:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": first_prompt},\n        ]\n    else:\n        messages = json.load(open(args.resume, \"r\"))\n        assert messages[-1][\"role\"] == \"user\", \"Last message must be user\"\n\n    t_start = time.time()\n    t_completion, t_train_start, t_train_end, t_eval_end = t_start, t_start, t_start, t_start\n    for i in range(args.num_generations):\n        # GENERATE CODE\n        if not args.do_baselines:\n            for _ in range(API_MAX_RETRY):\n                try:\n                    completion = openai.ChatCompletion.create(\n                        engine=gpt_model,\n                        messages=messages,\n                        max_tokens=2048,\n                        n=1,\n                        response_format={\"type\": \"json_object\"},\n                    ).choices[0]\n                    break\n                except openai.error.OpenAIError as e:\n                    print(type(e), e)\n                    time.sleep(API_RETRY_SLEEP)\n                except openai.error.InvalidRequestError as e:\n                    print(type(e), e)\n                    break\n            t_completion = time.time()\n\n            messages.append(completion.message.to_dict())\n            if not args.no_logging:\n                with open(f\"{save_dir}/messages.json\", \"w\") as f:\n                    json.dump(messages, f, indent=4)\n            out = json.loads(completion.message.content)\n        else:\n            out = {\"name\": archive[i][\"name\"], \"code\": archive[i][\"code\"]}\n\n        # VALIDATE CODE\n        valid, error = validate_code(out[\"code\"])\n        if not valid:\n            next_prompt = (\n                f\"Code not valid. Error:\\n{error}\\nPlease generate the next one.\"\n            )\n            messages.append({\"role\": \"user\", \"content\": next_prompt})\n            fitness = -1\n            print(\"CODE NOT VALID\")\n            continue\n        t_train_start = time.time()\n\n        # TRAIN GPO\n        trained, error = train_gpo(out, config)\n        if not trained:\n            next_prompt = (\n                f\"Training failed. Error:\\n{error}\\nPlease generate the next one.\"\n            )\n            messages.append({\"role\": \"user\", \"content\": next_prompt})\n            fitness = -1\n            print(\"FAILED TRAINING\")\n            continue\n        t_train_end = time.time()\n\n        # EVALUATE GPO\n        evaluated, val = evaluate_gpo(out, config)\n        if not evaluated:\n            next_prompt = (\n                f\"Evaluation failed. Error:\\n{val}\\nPlease generate the next one.\"\n            )\n            messages.append({\"role\": \"user\", \"content\": next_prompt})\n            fitness = -1\n            print(\"FAILED EVAL\")\n            continue\n        t_eval_end = time.time()\n\n        next_prompt = f\"Fitness: {val}.\\nPlease generate the next one.\"\n        messages.append({\"role\": \"user\", \"content\": next_prompt})\n        fitness = val",
    "Experiment Result": "LLM used for objective discovery: GPT-4. Initial objective functions provided to LLM (for 'burning-in'): DPO, HINGE, IPO, KTO, with pre-defined fitness values (e.g., 4.75 for DPO on 2B models, 7.887500 for DPO on 7B models). Number of generations (iterations) for objective discovery: 5 (default). Model parameters: 2 Billion (2B) or 7 Billion (7B). Number of GPUs used: 4 or 8. Downstream validation task: MT-Bench score, evaluated using FastChat's `gen_judgment.py`. Feedback mechanism: The MT-Bench score of the fine-tuned model is fed back to the GPT-4 LLM as in-context examples in the prompt for subsequent generations. Loss function interface: The LLM is prompted to propose a Python function with the signature `def your_loss_name(self, policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor) -> torch.FloatTensor:`. The `beta` parameter for the loss function is fixed to `self.beta = 0.05` within the LLM's context. Code validation includes checks for function callability, output shape, and absence of NaNs in the loss and its gradients."
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'The related paper has already proposed the use of token-level strictly proper scores (Brier, Spherical) instead of cross-entropy and explicitly reports the drawback that these scores converge slowly and yield worse perplexity when models are trained from scratch.  However, that paper leaves this drawback as an open limitation and does not present any concrete remedy.  The new method (CMSR) introduces a curriculum-based mixture loss that begins with cross-entropy and gradually shifts to a strictly proper rule, thereby attacking exactly this open problem.  To the best of current literature, no work has systematically scheduled the weight between CE and a strictly proper score during pre-training of language models; prior curriculum or multi-objective works mix CE with auxiliary objectives (e.g., RLHF rewards, contrastive losses) but not with alternative scoring rules specifically aimed at calibration.  The method is therefore conceptually simple yet novel in the specific context of calibration-oriented losses for LLMs.', 'novelty_score': 7, 'significance_reason': 'If the reported results hold, CMSR removes the principal obstacle that prevented strictly proper scoring rules from being used in full pre-training: slow convergence and higher perplexity.  Achieving almost CE-level perplexity while cutting Expected Calibration Error roughly in half would immediately benefit any downstream application that relies on probability estimates (decision-making, chain-of-thought selection, safety filters).  Because the change is a two-line modification with no extra hyper-parameters beyond a single schedule length, adoption cost is negligible for industry-scale LLM training pipelines.  Academically, the approach provides a practical bridge between classic information-theoretic scoring-rule theory and modern large-scale language model optimization, potentially spurring more work on dynamic loss scheduling for better calibrated generative models.  The improvement magnitude is moderate (+0.5 ROUGE-L, ~3.5% ECE), so the societal/academic impact, while meaningful, is not transformative.', 'significance_score': 7}
